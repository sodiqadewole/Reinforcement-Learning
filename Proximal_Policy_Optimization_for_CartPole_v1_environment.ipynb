{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Policy Optimization is a policy gradient method and can be used for environments with either discrete or continuous action spaces. It trains a stochastic policy in an on-policy way using an actor-critic method."
      ],
      "metadata": {
        "id": "0TwiC5CugsJ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D4Qu0zaMTKty"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gymnasium as gym\n",
        "import scipy.signal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def discounted_cumulative_sums(x, discount):\n",
        "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
        "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
        "\n",
        "\n",
        "class Buffer:\n",
        "    # Buffer for storing trajectories\n",
        "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
        "        # Buffer initialization\n",
        "        self.observation_buffer = np.zeros(\n",
        "            (size, observation_dimensions), dtype=np.float32\n",
        "        )\n",
        "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
        "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.gamma, self.lam = gamma, lam\n",
        "        self.pointer, self.trajectory_start_index = 0, 0\n",
        "\n",
        "    def store(self, observation, action, reward, value, logprobability):\n",
        "        # Append one step of agent-environment interaction\n",
        "        self.observation_buffer[self.pointer] = observation\n",
        "        self.action_buffer[self.pointer] = action\n",
        "        self.reward_buffer[self.pointer] = reward\n",
        "        self.value_buffer[self.pointer] = value\n",
        "        self.logprobability_buffer[self.pointer] = logprobability\n",
        "        self.pointer += 1\n",
        "\n",
        "    def finish_trajectory(self, last_value=0):\n",
        "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
        "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
        "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
        "        values = np.append(self.value_buffer[path_slice], last_value)\n",
        "\n",
        "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
        "\n",
        "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
        "            deltas, self.gamma * self.lam\n",
        "        )\n",
        "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
        "            rewards, self.gamma\n",
        "        )[:-1]\n",
        "\n",
        "        self.trajectory_start_index = self.pointer\n",
        "\n",
        "    def get(self):\n",
        "        # Get all data of the buffer and normalize the advantages\n",
        "        self.pointer, self.trajectory_start_index = 0, 0\n",
        "        advantage_mean, advantage_std = (\n",
        "            np.mean(self.advantage_buffer),\n",
        "            np.std(self.advantage_buffer),\n",
        "        )\n",
        "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
        "        return (\n",
        "            self.observation_buffer,\n",
        "            self.action_buffer,\n",
        "            self.advantage_buffer,\n",
        "            self.return_buffer,\n",
        "            self.logprobability_buffer,\n",
        "        )\n",
        "\n",
        "\n",
        "def mlp(x, sizes, activation=keras.activations.tanh, output_activation=None):\n",
        "    # Build a feedforward neural network\n",
        "    for size in sizes[:-1]:\n",
        "        x = layers.Dense(units=size, activation=activation)(x)\n",
        "    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
        "\n",
        "\n",
        "def logprobabilities(logits, a):\n",
        "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
        "    logprobabilities_all = keras.ops.log_softmax(logits)\n",
        "    logprobability = keras.ops.sum(\n",
        "        keras.ops.one_hot(a, num_actions) * logprobabilities_all, axis=1\n",
        "    )\n",
        "    return logprobability\n",
        "\n",
        "\n",
        "seed_generator = keras.random.SeedGenerator(1337)\n",
        "\n",
        "\n",
        "# Sample action from actor\n",
        "@tf.function\n",
        "def sample_action(observation):\n",
        "    logits = actor(observation)\n",
        "    action = keras.ops.squeeze(\n",
        "        keras.random.categorical(logits, 1, seed=seed_generator), axis=1\n",
        "    )\n",
        "    return logits, action\n",
        "\n",
        "\n",
        "# Train the policy by maxizing the PPO-Clip objective\n",
        "@tf.function\n",
        "def train_policy(\n",
        "    observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
        "):\n",
        "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
        "        ratio = keras.ops.exp(\n",
        "            logprobabilities(actor(observation_buffer), action_buffer)\n",
        "            - logprobability_buffer\n",
        "        )\n",
        "        min_advantage = keras.ops.where(\n",
        "            advantage_buffer > 0,\n",
        "            (1 + clip_ratio) * advantage_buffer,\n",
        "            (1 - clip_ratio) * advantage_buffer,\n",
        "        )\n",
        "\n",
        "        policy_loss = -keras.ops.mean(\n",
        "            keras.ops.minimum(ratio * advantage_buffer, min_advantage)\n",
        "        )\n",
        "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
        "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
        "\n",
        "    kl = keras.ops.mean(\n",
        "        logprobability_buffer\n",
        "        - logprobabilities(actor(observation_buffer), action_buffer)\n",
        "    )\n",
        "    kl = keras.ops.sum(kl)\n",
        "    return kl\n",
        "\n",
        "\n",
        "# Train the value function by regression on mean-squared error\n",
        "@tf.function\n",
        "def train_value_function(observation_buffer, return_buffer):\n",
        "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
        "        value_loss = keras.ops.mean((return_buffer - critic(observation_buffer)) ** 2)\n",
        "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
        "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))"
      ],
      "metadata": {
        "id": "WrLilvKSmBeD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "steps_per_epoch = 4000\n",
        "epochs = 30\n",
        "gamma = 0.99\n",
        "clip_ratio = 0.2\n",
        "policy_learning_rate = 3e-4\n",
        "value_function_learning_rate = 1e-3\n",
        "train_policy_iterations = 80\n",
        "train_value_iterations = 80\n",
        "lam = 0.97\n",
        "target_kl = 0.01\n",
        "hidden_sizes = (64, 64)\n",
        "\n",
        "render = False"
      ],
      "metadata": {
        "id": "jKRARh0BmBg7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XMExQnhtStCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the environment and get the dimensionality of the\n",
        "# observation space and the number of possible actions\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "observation_dimensions = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "# Initialize the buffer\n",
        "buffer = Buffer(observation_dimensions, steps_per_epoch)\n",
        "\n",
        "# Initialize the actor and the critic as keras models\n",
        "observation_input = keras.Input(shape=(observation_dimensions,), dtype=\"float32\")\n",
        "logits = mlp(observation_input, list(hidden_sizes) + [num_actions])\n",
        "actor = keras.Model(inputs=observation_input, outputs=logits)\n",
        "value = keras.ops.squeeze(mlp(observation_input, list(hidden_sizes) + [1]), axis=1)\n",
        "critic = keras.Model(inputs=observation_input, outputs=value)\n",
        "\n",
        "# Initialize the policy and the value function optimizers\n",
        "policy_optimizer = keras.optimizers.Adam(learning_rate=policy_learning_rate)\n",
        "value_optimizer = keras.optimizers.Adam(learning_rate=value_function_learning_rate)\n",
        "\n",
        "# Initialize the observation, episode return and episode length\n",
        "observation, _ = env.reset()\n",
        "episode_return, episode_length = 0, 0\n"
      ],
      "metadata": {
        "id": "SFXzGAzTmBju"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the number of epochs\n",
        "for epoch in range(epochs):\n",
        "    # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
        "    sum_return = 0\n",
        "    sum_length = 0\n",
        "    num_episodes = 0\n",
        "\n",
        "    # Iterate over the steps of each epoch\n",
        "    for t in range(steps_per_epoch):\n",
        "        if render:\n",
        "            env.render()\n",
        "\n",
        "        # Get the logits, action, and take one step in the environment\n",
        "        observation = observation.reshape(1, -1)\n",
        "        logits, action = sample_action(observation)\n",
        "        observation_new, reward, done, _, _ = env.step(action[0].numpy())\n",
        "        episode_return += reward\n",
        "        episode_length += 1\n",
        "\n",
        "        # Get the value and log-probability of the action\n",
        "        value_t = critic(observation)\n",
        "        logprobability_t = logprobabilities(logits, action)\n",
        "\n",
        "        # Store obs, act, rew, v_t, logp_pi_t\n",
        "        buffer.store(observation, action, reward, value_t, logprobability_t)\n",
        "\n",
        "        # Update the observation\n",
        "        observation = observation_new\n",
        "\n",
        "        # Finish trajectory if reached to a terminal state\n",
        "        terminal = done\n",
        "        if terminal or (t == steps_per_epoch - 1):\n",
        "            last_value = 0 if done else critic(observation.reshape(1, -1))\n",
        "            buffer.finish_trajectory(last_value)\n",
        "            sum_return += episode_return\n",
        "            sum_length += episode_length\n",
        "            num_episodes += 1\n",
        "            observation, _ = env.reset()\n",
        "            episode_return, episode_length = 0, 0\n",
        "\n",
        "    # Get values from the buffer\n",
        "    (\n",
        "        observation_buffer,\n",
        "        action_buffer,\n",
        "        advantage_buffer,\n",
        "        return_buffer,\n",
        "        logprobability_buffer,\n",
        "    ) = buffer.get()\n",
        "\n",
        "    # Update the policy and implement early stopping using KL divergence\n",
        "    for _ in range(train_policy_iterations):\n",
        "        kl = train_policy(\n",
        "            observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
        "        )\n",
        "        if kl > 1.5 * target_kl:\n",
        "            # Early Stopping\n",
        "            break\n",
        "\n",
        "    # Update the value function\n",
        "    for _ in range(train_value_iterations):\n",
        "        train_value_function(observation_buffer, return_buffer)\n",
        "\n",
        "    # Print mean return and length for each epoch\n",
        "    print(\n",
        "        f\" Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFnD9H1-mBmZ",
        "outputId": "eb1c0d3f-1413-4bcf-8a8c-751f45057565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch: 1. Mean Return: 32.0. Mean Length: 32.0\n",
            " Epoch: 2. Mean Return: 37.38317757009346. Mean Length: 37.38317757009346\n",
            " Epoch: 3. Mean Return: 53.333333333333336. Mean Length: 53.333333333333336\n",
            " Epoch: 4. Mean Return: 74.07407407407408. Mean Length: 74.07407407407408\n",
            " Epoch: 5. Mean Return: 133.33333333333334. Mean Length: 133.33333333333334\n",
            " Epoch: 6. Mean Return: 166.66666666666666. Mean Length: 166.66666666666666\n",
            " Epoch: 7. Mean Return: 200.0. Mean Length: 200.0\n",
            " Epoch: 8. Mean Return: 222.22222222222223. Mean Length: 222.22222222222223\n",
            " Epoch: 9. Mean Return: 160.0. Mean Length: 160.0\n",
            " Epoch: 10. Mean Return: 266.6666666666667. Mean Length: 266.6666666666667\n",
            " Epoch: 11. Mean Return: 363.6363636363636. Mean Length: 363.6363636363636\n",
            " Epoch: 12. Mean Return: 2000.0. Mean Length: 2000.0\n",
            " Epoch: 13. Mean Return: 800.0. Mean Length: 800.0\n",
            " Epoch: 14. Mean Return: 400.0. Mean Length: 400.0\n",
            " Epoch: 15. Mean Return: 1333.3333333333333. Mean Length: 1333.3333333333333\n",
            " Epoch: 16. Mean Return: 500.0. Mean Length: 500.0\n",
            " Epoch: 17. Mean Return: 1000.0. Mean Length: 1000.0\n",
            " Epoch: 18. Mean Return: 235.2941176470588. Mean Length: 235.2941176470588\n",
            " Epoch: 19. Mean Return: 266.6666666666667. Mean Length: 266.6666666666667\n",
            " Epoch: 20. Mean Return: 500.0. Mean Length: 500.0\n",
            " Epoch: 21. Mean Return: 571.4285714285714. Mean Length: 571.4285714285714\n",
            " Epoch: 22. Mean Return: 2000.0. Mean Length: 2000.0\n",
            " Epoch: 23. Mean Return: 666.6666666666666. Mean Length: 666.6666666666666\n",
            " Epoch: 24. Mean Return: 2000.0. Mean Length: 2000.0\n",
            " Epoch: 25. Mean Return: 1000.0. Mean Length: 1000.0\n",
            " Epoch: 26. Mean Return: 1000.0. Mean Length: 1000.0\n",
            " Epoch: 27. Mean Return: 1000.0. Mean Length: 1000.0\n",
            " Epoch: 28. Mean Return: 2000.0. Mean Length: 2000.0\n",
            " Epoch: 29. Mean Return: 4000.0. Mean Length: 4000.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dkRBHslFmBpV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}